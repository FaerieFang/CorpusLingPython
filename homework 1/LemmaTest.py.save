def lemmetize_print(words):
     from nltk.stem import WordNetLemmatizer
     from nltk.tokenize import word_tokenize
     lemmatizer = WordNetLemmatizer()
     a = []
     tokens = word_tokenize(words)
     for token in tokens:
          lemmetized_word = lemmatizer.lemmatize(token)
          a.append(lemmetized_word)
     sentence = " ".join(a)
     print(sentence)
import nltk
from nltk.corpus import wordnet
document = "I saw your face I see your faces"
sentences = nltk.word_tokenize(document)
lemmatizer = WordNetLemmatizer()
for key, val in freq.items():
    print(lemmatizer.lemmatize(key.lower(), pos="v"))
